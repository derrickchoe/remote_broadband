{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>city</th>\n",
       "      <th>afact</th>\n",
       "      <th>place_2000_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>1067</td>\n",
       "      <td>Abbeville</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL</td>\n",
       "      <td>1073</td>\n",
       "      <td>Adamsville</td>\n",
       "      <td>1.000</td>\n",
       "      <td>4965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AL</td>\n",
       "      <td>1117</td>\n",
       "      <td>Alabaster</td>\n",
       "      <td>1.000</td>\n",
       "      <td>22619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AL</td>\n",
       "      <td>1095</td>\n",
       "      <td>Albertville</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AL</td>\n",
       "      <td>1123</td>\n",
       "      <td>Alexander City</td>\n",
       "      <td>1.000</td>\n",
       "      <td>15008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  county            city  afact place_2000_pop\n",
       "1    AL    1067       Abbeville  1.000           2987\n",
       "2    AL    1073      Adamsville  1.000           4965\n",
       "5    AL    1117       Alabaster  1.000          22619\n",
       "6    AL    1095     Albertville  1.000          17247\n",
       "7    AL    1123  Alexander City  1.000          15008"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crosswalk from place to county\n",
    "# data link: http://mcdc.missouri.edu/cgi-bin/uexplore?/data/corrlst\n",
    "\n",
    "# PROBLEM: Nashville is a (balance) instead of city. Bethesda is a CDP instead of city. \n",
    "# Check for more cases like this and somehow fix\n",
    "\n",
    "xwalk = pd.read_csv('data/place_county.csv')\n",
    "# get cities and state 2dig abbrv\n",
    "xwalk = xwalk.loc[xwalk['PlaceName'].str.endswith('city')]\n",
    "xwalk['city'] = xwalk['PlaceName'].str.split(' city').str[0]\n",
    "xwalk['state'] = xwalk['CntyName'].str[-2:]\n",
    "xwalk['county'] = xwalk['county'].astype(int)\n",
    "xwalk['place_2000_pop'] = xwalk['pop100']\n",
    "# just keep necessary columns\n",
    "xwalk = xwalk.loc[:, ['state', 'county','city', 'afact', 'place_2000_pop']]\n",
    "xwalk.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>place_2000_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AK</td>\n",
       "      <td>Akhiok</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>Akiak</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AK</td>\n",
       "      <td>Akutan</td>\n",
       "      <td>713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AK</td>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AK</td>\n",
       "      <td>Allakaket</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state       city place_2000_pop\n",
       "0    AK     Akhiok             80\n",
       "1    AK      Akiak            309\n",
       "2    AK     Akutan            713\n",
       "3    AK  Aleknagik            221\n",
       "4    AK  Allakaket             97"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# population crosswalk\n",
    "xwalk_pop = xwalk.groupby(['state', 'city'])['place_2000_pop'].sum().reset_index()\n",
    "xwalk_pop.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCC data via Tedi-- thank you!\n",
    "# Subset data to keep DSL, cable, and fiber only\n",
    "# Only include consumer internet providers\n",
    "fccraw = pd.read_stata('data/US_2018_county.dta')\n",
    "cond = (fccraw['techcode'].isin([10, 11, 12, 20, 40, 41, 42, 43, 50])) & (fccraw['consumer'] == 1)\n",
    "fccraw = copy.deepcopy(fccraw[cond])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>w_avg_numproviders</th>\n",
       "      <th>avg_numproviders</th>\n",
       "      <th>median_numproviders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AK</td>\n",
       "      <td>Akhiok</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>Akiak</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AK</td>\n",
       "      <td>Akutan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AK</td>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AK</td>\n",
       "      <td>Allakaket</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state       city  w_avg_numproviders  avg_numproviders  median_numproviders\n",
       "0    AK     Akhiok                 3.0               3.0                  3.0\n",
       "1    AK      Akiak                 3.0               3.0                  3.0\n",
       "2    AK     Akutan                 1.0               1.0                  1.0\n",
       "3    AK  Aleknagik                 2.0               2.0                  2.0\n",
       "4    AK  Allakaket                 6.0               6.0                  6.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## frn codes don't map perfectly to provider name, but it just seems to be a renaming issue\n",
    "# x = (fcc.groupby('frn')['providername'].unique().str.len() > 1)\n",
    "# ntrue = x[x == 1]\n",
    "# fcc[fcc['frn'] == 4056248]\n",
    "\n",
    "## get number of providers by city, weighting by 2000 county population\n",
    "# number of providers per county\n",
    "fcc = fccraw.groupby('county')['frn'].nunique().reset_index()\n",
    "fcc = fcc.merge(xwalk, on = 'county')\n",
    "\n",
    "fcc['afact'] = fcc['afact'].astype(float)\n",
    "\n",
    "# weighted average function\n",
    "def wavg(df):\n",
    "    return((df['frn'] * df['afact']).sum().round())\n",
    "\n",
    "# weighted average number of providers by city\n",
    "fcc_gby = fcc.groupby(['state','city'])\n",
    "fcc = fcc_gby.apply(wavg).reset_index()\n",
    "fcc = fcc.rename(columns = {0: 'w_avg_numproviders'})\n",
    "fcc = fcc.merge(fcc_gby['frn'].mean().reset_index()\n",
    "                .rename(columns = {'frn': 'avg_numproviders'}), on = ['state', 'city'])\n",
    "fcc = fcc.merge(fcc_gby['frn'].median().reset_index()\n",
    "                .rename(columns = {'frn': 'median_numproviders'}), on = ['state', 'city'])\n",
    "\n",
    "fcc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of obs with city data: 13599\n",
      "Number of matched obs: 11233\n",
      "Unmerged GEOs\n",
      "['US-SOUTH-TN-Nashville' 'US-WEST-HI-Napili-Honokowai'\n",
      " 'US-NORTHEAST-NJ-Lakewood Township' 'US-SOUTH-VA-Tuckahoe'\n",
      " 'US-SOUTH-GA-Rex' 'US-WEST-ID-Boise' 'US-NORTHEAST-NY-Queensbury'\n",
      " 'US-NORTHEAST-PA-Huntingdon' 'US-MIDWEST-IN-Schererville'\n",
      " 'US-WEST-AZ-Marana' 'US-SOUTH-SC-Hilton Head Island'\n",
      " 'US-MIDWEST-WI-Plover' 'US-SOUTH-NC-Winston-Salem' 'US-SOUTH-AL-Shelby'\n",
      " 'US-WEST-CA-Goleta' 'US-SOUTH-LA-Sun' 'US-MIDWEST-MO-KCMO'\n",
      " 'US-SOUTH-DE-Pike Creek' 'US-WEST-AK-Anchorage'\n",
      " 'US-NORTHEAST-RI-Westerly' 'US-NORTHEAST-NY-Ballston'\n",
      " 'US-SOUTH-VA-Reston' 'US-WEST-HI-Wailuku' 'US-MIDWEST-IN-Indianapolis'\n",
      " 'US-MIDWEST-MI-Cassopolis' 'US-MIDWEST-MO-Affton' 'US-WEST-NV-Kingsbury'\n",
      " 'US-SOUTH-DE-Glasgow' 'US-MIDWEST-WI-Caledonia'\n",
      " 'US-SOUTH-TN-Collierville' 'US-WEST-AZ-Sahuarita'\n",
      " 'US-SOUTH-MD-Washington' 'US-NORTHEAST-ME-Falmouth'\n",
      " 'US-SOUTH-VA-Abingdon' 'US-MIDWEST-WI-Mount Pleasant'\n",
      " 'US-WEST-NV-Incline Village' 'US-WEST-AZ-Gilbert'\n",
      " 'US-MIDWEST-MI-Waterford Township' 'US-SOUTH-LA-Prairieville'\n",
      " 'US-WEST-HI-Kihei' 'US-SOUTH-AL-Grand Bay' 'US-WEST-NV-Carson City'\n",
      " 'US-WEST-CA-Ventura' 'US-NORTHEAST-NY-Fredonia' 'US-SOUTH-FL-Jupiter'\n",
      " 'US-NORTHEAST-PA-Lewistown' 'US-NORTHEAST-NY-Dansville'\n",
      " 'US-NORTHEAST-PA-Shippensburg' 'US-MIDWEST-IN-Westfield'\n",
      " 'US-SOUTH-TX-The Woodlands' 'US-MIDWEST-MI-Forest Hills'\n",
      " 'US-NORTHEAST-NJ-Brick Township' 'US-WEST-AZ-Buckeye' 'US-WEST-CO-Frisco'\n",
      " 'US-SOUTH-DE-Claymont' 'US-SOUTH-NC-Cary' 'US-SOUTH-SC-Edisto'\n",
      " 'US-SOUTH-DE-Brookside' 'US-SOUTH-VA-Washington' 'US-SOUTH-AL-Killen'\n",
      " 'US-WEST-CO-Windsor' 'US-SOUTH-FL-Myakka City' 'US-SOUTH-VA-Falmouth'\n",
      " 'US-WEST-HI-Honolulu' 'US-SOUTH-NC-Landis' 'US-NORTHEAST-NJ-Cherry Hill'\n",
      " 'US-NORTHEAST-PA-Bellefonte' 'US-SOUTH-FL-Davie'\n",
      " 'US-WEST-AZ-Casas Adobes' 'US-SOUTH-FL-North Naples' 'US-MIDWEST-IN-Avon'\n",
      " 'US-MIDWEST-OH-Ashville' 'US-MIDWEST-IN-Fishers' 'US-MIDWEST-IL-Oak Park'\n",
      " 'US-SOUTH-FL-Navarre' 'US-SOUTH-VA-Marion' 'US-MIDWEST-MO-Saint Charles'\n",
      " 'US-WEST-HI-Kahului' 'US-NORTHEAST-NJ-Washington Township'\n",
      " 'US-SOUTH-VA-Stuarts Draft' 'US-NORTHEAST-NY-Lake Placid'\n",
      " 'US-SOUTH-FL-The Villages' 'US-NORTHEAST-MA-Acton'\n",
      " 'US-MIDWEST-MO-Concord' 'US-WEST-AZ-Catalina Foothills'\n",
      " 'US-SOUTH-VA-Chesterfield' 'US-SOUTH-VA-Culpeper'\n",
      " 'US-NORTHEAST-PA-Mechanicsburg' 'US-SOUTH-FL-Palm City'\n",
      " 'US-SOUTH-GA-Athens' 'US-SOUTH-VA-Burke' 'US-MIDWEST-IN-Merrillville'\n",
      " 'US-SOUTH-FL-Rotonda West' 'US-NORTHEAST-MA-Plymouth'\n",
      " 'US-MIDWEST-IA-Omaha' 'US-SOUTH-NC-Kernersville' 'US-SOUTH-VA-Innsbrook'\n",
      " 'US-SOUTH-VA-Arlington' 'US-WEST-AZ-Sun City West'\n",
      " 'US-MIDWEST-MN-Saint Paul' 'US-SOUTH-LA-Ferriday'\n",
      " 'US-WEST-NV-Gardnerville Ranchos' 'US-NORTHEAST-CT-West Hartford'\n",
      " 'US-SOUTH-VA-Ashburn' 'US-NORTHEAST-NJ-Stafford Township'\n",
      " 'US-WEST-CO-Highlands Ranch' 'US-NORTHEAST-PA-Wilkes-Barre'\n",
      " 'US-NORTHEAST-NY-New Paltz' 'US-SOUTH-FL-Merritt Island'\n",
      " 'US-SOUTH-FL-Spring Hill' 'US-WEST-NM-Los Alamos' 'US-WEST-AZ-Maricopa'\n",
      " 'US-SOUTH-SC-Summerville' 'US-SOUTH-FL-Lakewood Ranch'\n",
      " 'US-SOUTH-VA-Mechanicsville' 'US-WEST-UT-Snyderville'\n",
      " 'US-MIDWEST-IL-South Elgin' 'US-NORTHEAST-NJ-Egg Harbor Township'\n",
      " 'US-WEST-AZ-Sun City' 'US-SOUTH-GA-Peachtree Corners'\n",
      " 'US-WEST-CA-El Dorado Hills' 'US-SOUTH-GA-Columbus'\n",
      " 'US-NORTHEAST-PA-Chevy Chase Heights' 'US-NORTHEAST-NJ-Edison'\n",
      " 'US-SOUTH-NC-Clemmons' 'US-SOUTH-VA-Spotsylvania Courthouse'\n",
      " 'US-SOUTH-KY-Lexington' 'US-NORTHEAST-ME-York' 'US-SOUTH-MS-Carriere'\n",
      " 'US-NORTHEAST-NJ-Galloway' 'US-NORTHEAST-NJ-East Brunswick'\n",
      " 'US-MIDWEST-MI-Meridian charter Township' 'US-MIDWEST-KS-KCMO'\n",
      " 'US-MIDWEST-WI-Cottage Grove' 'US-SOUTH-SC-Wilkinson Heights'\n",
      " 'US-NORTHEAST-NY-Massena' 'US-SOUTH-NC-River Road'\n",
      " 'US-NORTHEAST-PA-Milton' 'US-SOUTH-VA-Bon Air' 'US-MIDWEST-IL-Savoy'\n",
      " 'US-SOUTH-VA-Midlothian' 'US-NORTHEAST-NY-Saranac Lake'\n",
      " 'US-NORTHEAST-NJ-Hamilton Township' 'US-SOUTH-FL-Ruskin'\n",
      " 'US-MIDWEST-MO-Oakville' 'US-WEST-CA-Elk Grove' 'US-SOUTH-VA-Chantilly'\n",
      " 'US-NORTHEAST-NJ-Toms River' 'US-SOUTH-GA-Sandy Springs'\n",
      " 'US-WEST-AZ-Green Valley' 'US-NORTHEAST-CT-Windham'\n",
      " 'US-SOUTH-VA-Woodbridge' 'US-WEST-CO-Parker' 'US-SOUTH-VA-Glen Allen'\n",
      " 'US-SOUTH-VA-Christiansburg' 'US-MIDWEST-IL-Hoffman Estates'\n",
      " 'US-SOUTH-TN-Ooltewah' 'US-SOUTH-NC-Lewisville'\n",
      " 'US-MIDWEST-IL-Arlington Heights' 'US-NORTHEAST-CT-Newington'\n",
      " 'US-SOUTH-VA-Stafford Courthouse' 'US-MIDWEST-OH-Beckett Ridge'\n",
      " 'US-NORTHEAST-PA-Indiana' 'US-NORTHEAST-CT-Southington'\n",
      " 'US-WEST-CO-Breckenridge' 'US-NORTHEAST-CT-Cromwell'\n",
      " 'US-SOUTH-VA-Franconia' 'US-WEST-CA-Orcutt' 'US-MIDWEST-IL-Normal'\n",
      " 'US-SOUTH-GA-Augusta' 'US-SOUTH-VA-Annandale'\n",
      " 'US-NORTHEAST-PA-Monroeville' 'US-SOUTH-DE-Bear' 'US-WEST-CO-Centennial'\n",
      " 'US-MIDWEST-IN-Granger' 'US-NORTHEAST-CT-Montville'\n",
      " 'US-WEST-UT-Summit Park' 'US-SOUTH-FL-Miramar Beach'\n",
      " 'US-SOUTH-VA-Woodlake' 'US-MIDWEST-IL-Palatine' 'US-SOUTH-FL-Fruitville'\n",
      " 'US-SOUTH-AL-Chelsea' 'US-MIDWEST-IL-Lisle' 'US-NORTHEAST-NY-Getzville'\n",
      " 'US-NORTHEAST-NJ-Deptford Township' 'US-SOUTH-NC-Mooresville'\n",
      " 'US-SOUTH-MD-Elkridge' 'US-SOUTH-FL-Gulf Gate Estates' 'US-WEST-MT-Butte'\n",
      " 'US-WEST-CO-Castle Rock' 'US-MIDWEST-IL-Bourbonnais'\n",
      " 'US-NORTHEAST-CT-Farmington' 'US-SOUTH-VA-Hampden Sydney'\n",
      " 'US-SOUTH-LA-LaPlace' 'US-WEST-NV-Johnson Lane'\n",
      " 'US-MIDWEST-MI-Clinton Township' 'US-NORTHEAST-PA-State College'\n",
      " 'US-NORTHEAST-CT-Stonington' 'US-NORTHEAST-ME-Brunswick'\n",
      " 'US-SOUTH-FL-Bee Ridge' 'US-SOUTH-LA-Reserve' 'US-SOUTH-VA-Dale City'\n",
      " 'US-SOUTH-MD-Newark' 'US-MIDWEST-MO-Mehlville' 'US-NORTHEAST-PA-Ardmore'\n",
      " 'US-MIDWEST-MO-Old Jamestown' 'US-NORTHEAST-NY-Cheektowaga'\n",
      " 'US-MIDWEST-MI-Canton' 'US-WEST-CA-Alta Sierra' 'US-MIDWEST-IL-Morton'\n",
      " 'US-NORTHEAST-PA-Ross Township' 'US-WEST-WA-Spokane Valley'\n",
      " 'US-NORTHEAST-NJ-Little Egg Harbor Township' 'US-SOUTH-SC-Bamberg'\n",
      " 'US-NORTHEAST-NY-Irondequoit' 'US-WEST-AZ-Drexel Heights'\n",
      " 'US-NORTHEAST-NJ-Monroe Township'\n",
      " 'US-MIDWEST-MI-Port Huron charter Township' 'US-SOUTH-VA-Farmville'\n",
      " 'US-MIDWEST-OH-Boardman' 'US-SOUTH-GA-Johns Creek' 'US-SOUTH-VA-Chester'\n",
      " 'US-SOUTH-GA-Porterdale' 'US-SOUTH-VA-Cave Spring' 'US-SOUTH-MD-Bethesda'\n",
      " 'US-SOUTH-LA-Metairie' 'US-SOUTH-FL-Lutz' 'US-MIDWEST-WI-Waunakee'\n",
      " 'US-NORTHEAST-NJ-Secaucus' 'US-MIDWEST-IL-Schaumburg' 'US-WEST-CO-Granby'\n",
      " 'US-SOUTH-NC-Ahoskie' 'US-NORTHEAST-NJ-Fair Lawn' 'US-WEST-HI-Pukalani'\n",
      " 'US-WEST-AZ-Oro Valley' 'US-NORTHEAST-NJ-Princeton' 'US-SOUTH-GA-Tucker'\n",
      " 'US-SOUTH-VA-Patrick Springs' 'US-NORTHEAST-NJ-Parsippany-Troy Hills'\n",
      " 'US-SOUTH-FL-Sarasota Springs' 'US-SOUTH-AR-Bella Vista'\n",
      " 'US-NORTHEAST-CT-Plainville' 'US-SOUTH-MD-Potomac'\n",
      " 'US-SOUTH-NC-Oak Ridge' 'US-SOUTH-VA-Laurel' 'US-WEST-NV-Dayton'\n",
      " 'US-NORTHEAST-NJ-Franklin Township' 'US-SOUTH-SC-Brookdale'\n",
      " 'US-NORTHEAST-PA-Danville' 'US-WEST-NV-Paradise' 'US-SOUTH-FL-Pace'\n",
      " 'US-NORTHEAST-PA-Chambersburg' 'US-WEST-AZ-New River'\n",
      " 'US-MIDWEST-OH-Waverly' 'US-NORTHEAST-NJ-Roxbury Township'\n",
      " 'US-SOUTH-TX-Cypress']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>status</th>\n",
       "      <th>pub_cat</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>geo</th>\n",
       "      <th>weight</th>\n",
       "      <th>q</th>\n",
       "      <th>response_time</th>\n",
       "      <th>region</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>w_avg_numproviders</th>\n",
       "      <th>avg_numproviders</th>\n",
       "      <th>median_numproviders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100012282</td>\n",
       "      <td>2020-04-02 14:00:00</td>\n",
       "      <td>Complete</td>\n",
       "      <td>News</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>US-SOUTH-FL-Ocala</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None of the above / Not working for pay</td>\n",
       "      <td>9272</td>\n",
       "      <td>SOUTH</td>\n",
       "      <td>FL</td>\n",
       "      <td>Ocala</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1000296325</td>\n",
       "      <td>2020-04-02 03:00:00</td>\n",
       "      <td>Complete</td>\n",
       "      <td>News</td>\n",
       "      <td>Male</td>\n",
       "      <td>55-64</td>\n",
       "      <td>US-SOUTH-AL-Muscle Shoals</td>\n",
       "      <td>0.868838</td>\n",
       "      <td>I continue to commute to work</td>\n",
       "      <td>22582</td>\n",
       "      <td>SOUTH</td>\n",
       "      <td>AL</td>\n",
       "      <td>Muscle Shoals</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1000475325</td>\n",
       "      <td>2020-04-01 22:00:00</td>\n",
       "      <td>Complete</td>\n",
       "      <td>Reference</td>\n",
       "      <td>Female</td>\n",
       "      <td>55-64</td>\n",
       "      <td>US-MIDWEST-MI</td>\n",
       "      <td>0.626525</td>\n",
       "      <td>None of the above / Not working for pay</td>\n",
       "      <td>5775</td>\n",
       "      <td>MIDWEST</td>\n",
       "      <td>MI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1000492508</td>\n",
       "      <td>2020-04-02 15:00:00</td>\n",
       "      <td>Complete</td>\n",
       "      <td>News</td>\n",
       "      <td>Male</td>\n",
       "      <td>55-64</td>\n",
       "      <td>US-SOUTH-TN-Nashville</td>\n",
       "      <td>0.868838</td>\n",
       "      <td>I continue to commute to work</td>\n",
       "      <td>20659</td>\n",
       "      <td>SOUTH</td>\n",
       "      <td>TN</td>\n",
       "      <td>Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1000512424</td>\n",
       "      <td>2020-04-03 14:00:00</td>\n",
       "      <td>Complete</td>\n",
       "      <td>News</td>\n",
       "      <td>Female</td>\n",
       "      <td>45-54</td>\n",
       "      <td>US-SOUTH-TX</td>\n",
       "      <td>0.966719</td>\n",
       "      <td>Used to work from home and still do</td>\n",
       "      <td>32449</td>\n",
       "      <td>SOUTH</td>\n",
       "      <td>TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          id                 time    status    pub_cat   gender  \\\n",
       "0           1   100012282  2020-04-02 14:00:00  Complete       News  Unknown   \n",
       "1           2  1000296325  2020-04-02 03:00:00  Complete       News     Male   \n",
       "2           3  1000475325  2020-04-01 22:00:00  Complete  Reference   Female   \n",
       "3           4  1000492508  2020-04-02 15:00:00  Complete       News     Male   \n",
       "4           5  1000512424  2020-04-03 14:00:00  Complete       News   Female   \n",
       "\n",
       "       age                        geo    weight  \\\n",
       "0  Unknown          US-SOUTH-FL-Ocala  0.000000   \n",
       "1    55-64  US-SOUTH-AL-Muscle Shoals  0.868838   \n",
       "2    55-64              US-MIDWEST-MI  0.626525   \n",
       "3    55-64      US-SOUTH-TN-Nashville  0.868838   \n",
       "4    45-54                US-SOUTH-TX  0.966719   \n",
       "\n",
       "                                         q  response_time   region state  \\\n",
       "0  None of the above / Not working for pay           9272    SOUTH    FL   \n",
       "1            I continue to commute to work          22582    SOUTH    AL   \n",
       "2  None of the above / Not working for pay           5775  MIDWEST    MI   \n",
       "3            I continue to commute to work          20659    SOUTH    TN   \n",
       "4      Used to work from home and still do          32449    SOUTH    TX   \n",
       "\n",
       "            city  w_avg_numproviders  avg_numproviders  median_numproviders  \n",
       "0          Ocala                10.0              10.0                 10.0  \n",
       "1  Muscle Shoals                 5.0               5.0                  5.0  \n",
       "2            NaN                 NaN               NaN                  NaN  \n",
       "3      Nashville                 NaN               NaN                  NaN  \n",
       "4            NaN                 NaN               NaN                  NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Work from home data\n",
    "work = pd.read_csv('data/horton_data/gcs-apr5_w_states.csv')\n",
    "# city is the 4th geographic identifier in the \"geo\" column\n",
    "work['city'] = work['geo'].str.split('-').str[3]\n",
    "\n",
    "# count number of observations with city data-- 13599/25K. 54% of sample\n",
    "print(\"Number of obs with city data: {}\".format(work['city'].notnull().sum()))\n",
    "\n",
    "# count number of cities matched to fcc data\n",
    "merged = work.merge(fcc, on = ['state', 'city'], how = 'left')\n",
    "print(\"Number of matched obs: {}\".format(merged['avg_numproviders'].notnull().sum()))\n",
    "\n",
    "# unmatched cities:\n",
    "print('Unmerged GEOs')\n",
    "print(merged[(merged['avg_numproviders'].isnull()) & (merged['city'].notnull())]['geo'].unique())\n",
    "\n",
    "merged.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>avg_numproviders</th>\n",
       "      <th>median_numproviders</th>\n",
       "      <th>None of the above / Not working for pay</th>\n",
       "      <th>I continue to commute to work</th>\n",
       "      <th>Used to work from home and still do</th>\n",
       "      <th>Used to commute, now work from home</th>\n",
       "      <th>I have recently been furloughed or laid-off</th>\n",
       "      <th>Used to work from home, but now I commute</th>\n",
       "      <th>place_2000_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AK</td>\n",
       "      <td>Anchorage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>Wasilla</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL</td>\n",
       "      <td>Alabaster</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AL</td>\n",
       "      <td>Andalusia</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL</td>\n",
       "      <td>Atmore</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AL</td>\n",
       "      <td>Auburn</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AL</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>36.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242307513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AL</td>\n",
       "      <td>Calera</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>307979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AL</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AL</td>\n",
       "      <td>Clanton</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state        city  avg_numproviders  median_numproviders  \\\n",
       "0    AK   Anchorage               NaN                  NaN   \n",
       "1    AK     Wasilla               3.0                  3.0   \n",
       "2    AL   Alabaster               4.0                  4.0   \n",
       "3    AL   Andalusia               5.0                  5.0   \n",
       "4    AL      Atmore               6.0                  6.0   \n",
       "5    AL      Auburn               8.0                  8.0   \n",
       "6    AL  Birmingham               6.5                  6.5   \n",
       "7    AL      Calera               4.5                  4.5   \n",
       "8    AL     Chelsea               NaN                  NaN   \n",
       "9    AL     Clanton               5.0                  5.0   \n",
       "\n",
       "   None of the above / Not working for pay  I continue to commute to work  \\\n",
       "0                                     12.0                            7.0   \n",
       "1                                      5.0                            0.0   \n",
       "2                                      9.0                            6.0   \n",
       "3                                      3.0                            1.0   \n",
       "4                                      3.0                            2.0   \n",
       "5                                      4.0                            4.0   \n",
       "6                                     36.0                           30.0   \n",
       "7                                      5.0                            2.0   \n",
       "8                                      3.0                            1.0   \n",
       "9                                      4.0                            0.0   \n",
       "\n",
       "   Used to work from home and still do  Used to commute, now work from home  \\\n",
       "0                                  1.0                                  6.0   \n",
       "1                                  0.0                                  2.0   \n",
       "2                                  2.0                                  2.0   \n",
       "3                                  0.0                                  1.0   \n",
       "4                                  0.0                                  1.0   \n",
       "5                                  2.0                                  2.0   \n",
       "6                                  5.0                                 13.0   \n",
       "7                                  1.0                                  0.0   \n",
       "8                                  2.0                                  1.0   \n",
       "9                                  0.0                                  1.0   \n",
       "\n",
       "   I have recently been furloughed or laid-off  \\\n",
       "0                                          1.0   \n",
       "1                                          1.0   \n",
       "2                                          1.0   \n",
       "3                                          0.0   \n",
       "4                                          0.0   \n",
       "5                                          0.0   \n",
       "6                                          5.0   \n",
       "7                                          0.0   \n",
       "8                                          0.0   \n",
       "9                                          0.0   \n",
       "\n",
       "   Used to work from home, but now I commute place_2000_pop  \n",
       "0                                        0.0            NaN  \n",
       "1                                        0.0           5469  \n",
       "2                                        0.0          22619  \n",
       "3                                        0.0           8794  \n",
       "4                                        0.0           7676  \n",
       "5                                        0.0          42987  \n",
       "6                                        2.0      242307513  \n",
       "7                                        0.0         307979  \n",
       "8                                        0.0            NaN  \n",
       "9                                        0.0           7800  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get number working numbers by city\n",
    "\n",
    "# indicator for type of survey respondent\n",
    "qs = list(merged['q'].unique())\n",
    "for q in merged['q'].unique():\n",
    "    merged[q] = merged['q'] == q\n",
    "    \n",
    "# create final city-level dataset: number of providers, number of respondents not working, working remotely, etc. \n",
    "mgby = merged.groupby(['state', 'city'])\n",
    "final = mgby[['avg_numproviders', 'median_numproviders']].max().reset_index()\n",
    "final = final.merge(\n",
    "    mgby[qs].sum().reset_index(),\n",
    "    on = ['state','city'])\n",
    "\n",
    "# merge population data\n",
    "final = final.merge(xwalk_pop, on = ['state', 'city'], how = 'left')\n",
    "\n",
    "final.to_csv('data/city_broadband_work.csv')\n",
    "final.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
